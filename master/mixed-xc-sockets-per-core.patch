Expose vcpus as multiple cores in a smaller number of sockets,
by adjusting the cpuid responses appropriately.

diff --git a/tools/libxc/include/xenctrl.h b/tools/libxc/include/xenctrl.h
index e05ddb1..2c1eefa 100644
--- a/tools/libxc/include/xenctrl.h
+++ b/tools/libxc/include/xenctrl.h
@@ -1340,6 +1340,10 @@ int xc_domain_get_tsc_info(xc_interface *xch,
 
 int xc_domain_disable_migrate(xc_interface *xch, uint32_t domid);
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket);
+
 int xc_domain_maximum_gpfn(xc_interface *xch, domid_t domid);
 
 int xc_domain_increase_reservation(xc_interface *xch,
diff --git a/tools/libxc/xc_domain.c b/tools/libxc/xc_domain.c
index 0f2af23..4f9f3f4 100644
--- a/tools/libxc/xc_domain.c
+++ b/tools/libxc/xc_domain.c
@@ -773,6 +773,20 @@ int xc_domain_get_tsc_info(xc_interface *xch,
 }
 
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket)
+{
+    struct xen_domctl domctl =
+    {
+        .cmd = XEN_DOMCTL_setcorespersocket,
+        .domain = domid,
+        .u.corespersocket.cores_per_socket = cores_per_socket,
+    };
+
+    return do_domctl(xch, &domctl);
+}
+
 int xc_domain_maximum_gpfn(xc_interface *xch, domid_t domid)
 {
     return do_memory_op(xch, XENMEM_maximum_gpfn, &domid, sizeof(domid));
diff --git a/xen/arch/x86/hvm/svm/svm.c b/xen/arch/x86/hvm/svm/svm.c
index bc8a0d3..4e97d65 100644
--- a/xen/arch/x86/hvm/svm/svm.c
+++ b/xen/arch/x86/hvm/svm/svm.c
@@ -1539,14 +1539,52 @@ static void svm_cpuid_intercept(
 {
     unsigned int input = *eax;
     struct vcpu *v = current;
+    unsigned int cps = v->domain->cores_per_socket;
 
     hvm_cpuid(input, eax, ebx, ecx, edx);
 
     switch (input) {
+    case 1:
+        if ( cps > 0 )
+        {
+            /* to fake out #vcpus per socket first force on HT/MC */
+            *edx |= cpufeat_mask(X86_FEATURE_HT);
+            /* fake out #vcpus and inform guest of #cores per package */
+            *ebx &= 0xFF00FFFF;
+            /*
+             * This (cps * 2) is wrong, and contrary to the statement in the
+             * AMD manual.  However, Xen unconditionally offers Intel-style
+             * APIC IDs (odd IDs for hyperthreads) which breaks the AMD APIC
+             * Enumeration Requirements.
+             *
+             * Fake up cores-per-socket as a socket with twice as many cores
+             * as expected, with every odd core offline.
+             */
+            *ebx |= (((cps * 2) & 0xFF) << 16);
+        }
+        break;
     case 0x80000001:
         /* Fix up VLAPIC details. */
         if ( vlapic_hw_disabled(vcpu_vlapic(v)) )
             __clear_bit(X86_FEATURE_APIC & 31, edx);
+        if ( cps > 0 )
+            *ecx |= cpufeat_mask(X86_FEATURE_CMP_LEGACY);
+        break;
+    case 0x80000008:
+        if ( cps > 0 )
+        {
+            *ecx &= 0xFFFF0F00;
+            /*
+             * This (cps * 2) is wrong, and contrary to the statement in the
+             * AMD manual.  However, Xen unconditionally offers Intel-style
+             * APIC IDs (odd IDs for hyperthreads) which breaks the AMD APIC
+             * Enumeration Requirements.
+             *
+             * Fake up cores-per-socket as a socket with twice as many cores
+             * as expected, with every odd core offline.
+             */
+            *ecx |= ((cps * 2) - 1) & 0xFF;
+        }
         break;
     case 0x8000001c: 
     {
diff --git a/xen/arch/x86/hvm/vmx/vmx.c b/xen/arch/x86/hvm/vmx/vmx.c
index 304aeea..3342015 100644
--- a/xen/arch/x86/hvm/vmx/vmx.c
+++ b/xen/arch/x86/hvm/vmx/vmx.c
@@ -1862,11 +1862,32 @@ static void vmx_cpuid_intercept(
     unsigned int input = *eax;
     struct segment_register cs;
     struct vcpu *v = current;
+    unsigned int cps = v->domain->cores_per_socket;
 
     hvm_cpuid(input, eax, ebx, ecx, edx);
 
     switch ( input )
     {
+        case 0x00000001:
+            if ( cps > 0 )
+            {
+                /* to fake out #vcpus per socket first force on HT/MC */
+                *edx |= cpufeat_mask(X86_FEATURE_HT);
+                /* fake out #vcpus and inform guest of #cores per package */
+                *ebx &= 0xFF00FFFF;
+                *ebx |= (((cps * 2) & 0xFF) << 16);
+            }
+            break;
+
+        case 0x00000004:
+            if ( cps > 0 )
+            {
+                /* fake out cores per socket */
+                *eax &= 0x3FFF; /* one thread, one core */
+                *eax |= (((cps * 2) - 1) << 26);
+            }
+            break;
+
         case 0x80000001:
             /* SYSCALL is visible iff running in long mode. */
             vmx_get_segment_register(v, x86_seg_cs, &cs);
diff --git a/xen/common/domctl.c b/xen/common/domctl.c
index fdd4845..01ba00d 100644
--- a/xen/common/domctl.c
+++ b/xen/common/domctl.c
@@ -1195,6 +1195,44 @@ long do_domctl(XEN_GUEST_HANDLE_PARAM(xen_domctl_t) u_domctl)
         break;
     }
 
+    case XEN_DOMCTL_setcorespersocket:
+    {
+        struct domain *d;
+
+        ret = -ESRCH;
+        d = rcu_lock_domain_by_id(op->domain);
+
+        if ( d != NULL )
+        {
+            unsigned int cps = op->u.corespersocket.cores_per_socket;
+
+            /* Toolstack is permitted to set this value exactly once. */
+            if ( d->cores_per_socket != 0 )
+                ret = -EEXIST;
+
+            /* Only meaningful for HVM domains. */
+            else if ( !has_hvm_container_domain(d) )
+                ret = -EOPNOTSUPP;
+
+            /* Cores per socket is strictly within the bounds of max_vcpus. */
+            else if ( cps < 1 || cps > d->max_vcpus )
+                ret = -EINVAL;
+
+            /* Cores per socket must exactly divide max_vcpus. */
+            else if ( d->max_vcpus % cps != 0 )
+                ret = -EDOM;
+
+            else
+            {
+                d->cores_per_socket = cps;
+                ret = 0;
+            }
+
+            rcu_unlock_domain(d);
+        }
+    }
+    break;
+
     default:
         ret = arch_do_domctl(op, d, u_domctl);
         break;
diff --git a/xen/include/public/domctl.h b/xen/include/public/domctl.h
index 969c65f..365913b 100644
--- a/xen/include/public/domctl.h
+++ b/xen/include/public/domctl.h
@@ -1011,6 +1011,13 @@ DEFINE_XEN_GUEST_HANDLE(xen_domctl_runstate_info_t);
 /* Some vcpus are runnable, some are blocked */
 #define DOMAIN_RUNSTATE_partial_contention 5
 
+struct xen_domctl_corespersocket {
+    uint32_t cores_per_socket;
+};
+
+typedef struct xen_domctl_corespersocket xen_domctl_corespersocket_t;
+DEFINE_XEN_GUEST_HANDLE(xen_domctl_corespersocket_t);
+
 struct xen_domctl {
     uint32_t cmd;
 #define XEN_DOMCTL_createdomain                   1
@@ -1089,6 +1096,7 @@ struct xen_domctl {
 #define XEN_DOMCTL_gdbsx_pausevcpu             1001
 #define XEN_DOMCTL_gdbsx_unpausevcpu           1002
 #define XEN_DOMCTL_gdbsx_domstatus             1003
+#define XEN_DOMCTL_setcorespersocket           4001
     uint32_t interface_version; /* XEN_DOMCTL_INTERFACE_VERSION */
     domid_t  domain;
     union {
@@ -1141,6 +1149,7 @@ struct xen_domctl {
         struct xen_domctl_set_virq_handler  set_virq_handler;
         struct xen_domctl_set_max_evtchn    set_max_evtchn;
         struct xen_domctl_runstate_info     domain_runstate;
+        struct xen_domctl_corespersocket    corespersocket;
         struct xen_domctl_gdbsx_memio       gdbsx_guest_memio;
         struct xen_domctl_set_broken_page_p2m set_broken_page_p2m;
         struct xen_domctl_cacheflush        cacheflush;
diff --git a/xen/include/xen/sched.h b/xen/include/xen/sched.h
index fc65bd3..e188fe3 100644
--- a/xen/include/xen/sched.h
+++ b/xen/include/xen/sched.h
@@ -463,6 +463,8 @@ struct domain
     spinlock_t runstate_lock;
     atomic_t runstate_missed_changes;
     domain_runstate_info_t runstate;
+
+    unsigned int cores_per_socket;
 };
 
 struct domain_setup_info
